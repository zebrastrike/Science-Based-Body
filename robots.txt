# Robots.txt for Science Based Body
# https://sciencebasedbody.com

# Allow all crawlers
User-agent: *

# Allow indexing of all public pages
Allow: /

# Disallow admin and private areas
Disallow: /apps/
Disallow: /node_modules/
Disallow: /config/
Disallow: /scripts/
Disallow: /ssl/
Disallow: /.git/
Disallow: /.claude/

# Disallow temporary and system files
Disallow: /*.log$
Disallow: /*.tmp$
Disallow: /*.bak$

# Crawl delay (optional - be polite to servers)
Crawl-delay: 1

# Sitemap location
Sitemap: https://sciencebasedbody.com/sitemap.xml

# Google specific
User-agent: Googlebot
Allow: /
Crawl-delay: 0

# Bing specific
User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Block AI training crawlers (optional - uncomment if desired)
# User-agent: GPTBot
# Disallow: /
# User-agent: CCBot
# Disallow: /
# User-agent: anthropic-ai
# Disallow: /
